{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_gpu = torch.cuda.is_available()\n",
    "if cuda_gpu:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu:0\")\n",
    "nclass = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.wide_resnet101_2(pretrained=True)\n",
    "model.fc = nn.Linear(2048, 100, bias=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in model.named_parameters():\n",
    "    if \"fc\" not in name and \"classifier\" not in name:\n",
    "        value.requires_grad = False\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "SAVE_EPOCH = 5\n",
    "VAL_EPOCH = 10\n",
    "verbose = 500\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer = transforms.Compose([\n",
    "    #transforms.RandomResizedCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize([124,117,104],[58.4,57.1,57.4])\n",
    "    ])\n",
    "\n",
    "class ImageNetData(Dataset):\n",
    "    \n",
    "    def __init__(self, path, info, nclass):\n",
    "        self.path = path\n",
    "        info_path = os.path.join(path, info)\n",
    "        f = open(info_path)\n",
    "        self.data = f.read().splitlines()\n",
    "        self.transforms = data_transformer\n",
    "        self.nclass = nclass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.path, self.data[index].split(' ')[0]))\n",
    "        img = self.transforms(img)\n",
    "        label = self.data[index].split(' ')[1]\n",
    "        label = int(label)\n",
    "#         one_hot = np.zeros(self.nclass, dtype=np.int)\n",
    "#         one_hot[label] = 1\n",
    "        return (img, label)\n",
    "\n",
    "path = \"D:/课程文件/大四上/深度学习导论/作业/5/TinyImageNet/TinyImageNet\"\n",
    "train_info_path = path + '/train.txt'\n",
    "val_info_path = path + '/val.txt'\n",
    "train_set = ImageNetData(path, \"train.txt\", nclass)\n",
    "val_set = ImageNetData(path, \"val.txt\", nclass)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckptpath = \"./ckpts/\"\n",
    "ckpts = sorted([x for x in os.listdir(ckptpath) if x.endswith(\".ckpt\")])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "lr_reduct = lambda epochs: 0.5 ** (epochs // 10)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_reduct)\n",
    "if len(ckpts) >= 1:\n",
    "    state_dicts = torch.load(ckptpath+ckpts[-1])\n",
    "    model.load_state_dict(state_dicts['model'])\n",
    "    optimizer.load_state_dict(state_dicts['optimizer'])\n",
    "    scheduler.load_state_dict(state_dicts['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "Train: 500/1563 - Average Time: 1.3955431389808655\n",
      "Train: 1000/1563 - Average Time: 1.4227632761001587\n",
      "Train: 1500/1563 - Average Time: 1.4254892638524372\n",
      "End of Train Epoch: 1 - Average Time: 1.4229241120685618\n",
      "Train loss : 0.6246591548144017 Train accuracy : 0.8219369801663468\n",
      "37m 4s\n",
      "Epoch 2/30\n",
      "Train: 500/1563 - Average Time: 1.6698806338310241\n",
      "Train: 1000/1563 - Average Time: 1.555335125684738\n",
      "Train: 1500/1563 - Average Time: 1.512894520441691\n",
      "End of Train Epoch: 2 - Average Time: 1.5070908470788371\n",
      "Train loss : 0.5813210093567025 Train accuracy : 0.8336332373640435\n",
      "39m 16s\n",
      "Epoch 3/30\n",
      "Train: 500/1563 - Average Time: 1.4263433737754823\n",
      "Train: 1000/1563 - Average Time: 1.4594030947685241\n",
      "Train: 1500/1563 - Average Time: 1.4878497721354167\n",
      "End of Train Epoch: 3 - Average Time: 1.4894652805950728\n",
      "Train loss : 0.5787250924419304 Train accuracy : 0.8346529110684581\n",
      "38m 48s\n",
      "Epoch 4/30\n",
      "Train: 500/1563 - Average Time: 1.510214578151703\n",
      "Train: 1000/1563 - Average Time: 1.5059459121227265\n",
      "Train: 1500/1563 - Average Time: 1.4735426858266194\n",
      "End of Train Epoch: 4 - Average Time: 1.4695208138254157\n",
      "Train loss : 0.5747384797073829 Train accuracy : 0.8359924824056302\n",
      "38m 17s\n",
      "Epoch 5/30\n",
      "Train: 500/1563 - Average Time: 1.4124802923202515\n",
      "Train: 1000/1563 - Average Time: 1.4129797296524047\n",
      "Train: 1500/1563 - Average Time: 1.41298676888148\n",
      "End of Train Epoch: 5 - Average Time: 1.4148126865760378\n",
      "Train loss : 0.5751170548619327 Train accuracy : 0.8347228886756238\n",
      "36m 53s\n",
      "Epoch 6/30\n",
      "Train: 500/1563 - Average Time: 1.4252968220710753\n",
      "Train: 1000/1563 - Average Time: 1.422608232498169\n",
      "Train: 1500/1563 - Average Time: 1.4172064468065897\n",
      "End of Train Epoch: 6 - Average Time: 1.41452964635057\n",
      "Train loss : 0.5720395191274082 Train accuracy : 0.8349428182981445\n",
      "36m 51s\n",
      "Epoch 7/30\n",
      "Train: 500/1563 - Average Time: 1.4057076044082641\n",
      "Train: 1000/1563 - Average Time: 1.4031179418563844\n",
      "Train: 1500/1563 - Average Time: 1.403192062854767\n",
      "End of Train Epoch: 7 - Average Time: 1.4013399103095114\n",
      "Train loss : 0.5687190686672525 Train accuracy : 0.8363323736404351\n",
      "36m 30s\n",
      "Epoch 8/30\n",
      "Train: 500/1563 - Average Time: 1.4055287108421326\n",
      "Train: 1000/1563 - Average Time: 1.4047449555397034\n",
      "Train: 1500/1563 - Average Time: 1.4060787901878358\n",
      "End of Train Epoch: 8 - Average Time: 1.4079235960715715\n",
      "Train loss : 0.5732857417732343 Train accuracy : 0.8369021912987844\n",
      "36m 41s\n",
      "Epoch 9/30\n",
      "Train: 500/1563 - Average Time: 1.4081435141563416\n",
      "Train: 1000/1563 - Average Time: 1.4116107201576233\n",
      "Train: 1500/1563 - Average Time: 1.4100071651140849\n",
      "End of Train Epoch: 9 - Average Time: 1.4084209745996517\n",
      "Train loss : 0.565377138893496 Train accuracy : 0.8383117402431222\n",
      "36m 41s\n",
      "Epoch 10/30\n",
      "Train: 500/1563 - Average Time: 1.4062907080650329\n",
      "Train: 1000/1563 - Average Time: 1.407061074256897\n",
      "Train: 1500/1563 - Average Time: 1.4056031325658163\n",
      "End of Train Epoch: 10 - Average Time: 1.404009431123886\n",
      "Train loss : 0.5678828217170212 Train accuracy : 0.8365623000639796\n",
      "Val   loss : 7.172381777672252 Val   accuracy : 0.08439490445859872\n",
      "40m 21s\n",
      "Epoch 11/30\n",
      "Train: 500/1563 - Average Time: 1.400133915901184\n",
      "Train: 1000/1563 - Average Time: 1.3975376455783843\n",
      "Train: 1500/1563 - Average Time: 1.3966841665903726\n",
      "End of Train Epoch: 11 - Average Time: 1.3983999190998626\n",
      "Train loss : 0.5644944405048533 Train accuracy : 0.838531669865643\n",
      "36m 26s\n",
      "Epoch 12/30\n",
      "Train: 500/1563 - Average Time: 1.3916388468742371\n",
      "Train: 1000/1563 - Average Time: 1.3935692563056945\n",
      "Train: 1500/1563 - Average Time: 1.3937178931236267\n",
      "End of Train Epoch: 12 - Average Time: 1.3940859007209978\n",
      "Train loss : 0.5454772416495087 Train accuracy : 0.8430702175303902\n",
      "36m 19s\n",
      "Epoch 13/30\n",
      "Train: 500/1563 - Average Time: 1.392237181186676\n",
      "Train: 1000/1563 - Average Time: 1.3931280341148375\n",
      "Train: 1500/1563 - Average Time: 1.392476530234019\n",
      "End of Train Epoch: 13 - Average Time: 1.3939410839337076\n",
      "Train loss : 0.5435152071375002 Train accuracy : 0.8428902751119641\n",
      "36m 19s\n",
      "Epoch 14/30\n",
      "Train: 500/1563 - Average Time: 1.3913580255508422\n",
      "Train: 1000/1563 - Average Time: 1.3939208257198334\n",
      "Train: 1500/1563 - Average Time: 1.3955105120340983\n",
      "End of Train Epoch: 14 - Average Time: 1.3941822799443435\n",
      "Train loss : 0.5448634277740809 Train accuracy : 0.8434101087651952\n",
      "36m 19s\n",
      "Epoch 15/30\n",
      "Train: 500/1563 - Average Time: 1.4004244589805603\n",
      "Train: 1000/1563 - Average Time: 1.3993015019893646\n",
      "Train: 1500/1563 - Average Time: 1.3998492132822673\n",
      "End of Train Epoch: 15 - Average Time: 1.399218770379221\n",
      "Train loss : 0.5453013613371992 Train accuracy : 0.8440399072296865\n",
      "36m 29s\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-286b90444a79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epochs in range(1, EPOCHS + 1):\n",
    "        print('Epoch {}/{}'.format(epochs, EPOCHS))\n",
    "        start_time = time.time()\n",
    "        running_train_loss = []\n",
    "        running_train_acc = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs = data[0].to(device)\n",
    "            targets = data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = outputs.argmax(axis=1)\n",
    "            running_train_loss.append(float(loss))\n",
    "            running_train_acc.append(float((pred==targets).sum()) / BATCH_SIZE)\n",
    "            if verbose > 0 and (i + 1) % verbose == 0:\n",
    "                print(\"Train: {}/{} - Average Time: {}\".format(i + 1, len(train_loader), (time.time()-start_time) / (i + 1)))\n",
    "        print(\"End of Train Epoch: {} - Average Time: {}\".format(epochs, (time.time() - start_time) / (i + 1)))\n",
    "        # 周期性清除CUDA缓存\n",
    "        torch.cuda.empty_cache()\n",
    "        if epochs % SAVE_EPOCH == 0:\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                       'optimizer': optimizer.state_dict(), \n",
    "                       'scheduler': scheduler.state_dict()},\n",
    "                       ckptpath + \"model_{}.ckpt\".format(int(datetime.datetime.now().timestamp())))\n",
    "        if epochs % VAL_EPOCH == 0:\n",
    "            running_val_loss = []\n",
    "            running_val_acc = []\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs=data[0].to(device)\n",
    "                targets=data[1].to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, targets)\n",
    "                    pred = outputs.argmax(axis=1)\n",
    "                    running_val_loss.append(float(loss))\n",
    "                    running_val_acc.append(float((pred==targets).sum()) / BATCH_SIZE)\n",
    "                if verbose > 0 and(i + 1) % verbose == 0:\n",
    "                    print(\"Val: {}/{}\".format(i + 1, len(val_loader)))\n",
    "        \n",
    "        scheduler.step()    \n",
    "        train_loss.append(sum(running_train_loss) / len(running_train_loss))\n",
    "        train_acc.append(sum(running_train_acc) / len(running_train_acc))\n",
    "        duration = time.time() - start_time\n",
    "        print('Train loss : {} Train accuracy : {}'.format(train_loss[-1], train_acc[-1]))\n",
    "        if epochs % VAL_EPOCH == 0:\n",
    "            val_loss.append(sum(running_val_loss) / len(running_val_loss))\n",
    "            val_acc.append(sum(running_val_acc) / len(running_val_acc))\n",
    "            print('Val   loss : {} Val   accuracy : {}'.format(val_loss[-1], val_acc[-1]))\n",
    "        print('{:.0f}m {:.0f}s'.format(duration // 60, duration % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model.state_dict(), \n",
    "            'optimizer': optimizer.state_dict(), \n",
    "            'scheduler': scheduler.state_dict()},\n",
    "            ckptpath + \"model_{}.ckpt\".format(int(datetime.datetime.now().timestamp())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model_1604847063.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "ckpts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2020, tm_mon=11, tm_mday=11, tm_hour=7, tm_min=42, tm_sec=53, tm_wday=2, tm_yday=316, tm_isdst=0)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "time.localtime(1605051773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataTest(Dataset):\n",
    "    def __init__(self, path, info, nclass):\n",
    "        self.path = path\n",
    "        info_path = os.path.join(path, info)\n",
    "        f = open(info_path)\n",
    "        self.data = f.read().splitlines()\n",
    "        self.transforms = data_transformer\n",
    "        self.nclass = nclass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.path, self.data[index].split(' ')[0]))\n",
    "        img = self.transforms(img)\n",
    "#         one_hot = np.zeros(self.nclass, dtype=np.int)\n",
    "#         one_hot[label] = 1\n",
    "        return (self.data[index], img)\n",
    "\n",
    "test_info_path = path + '/test.txt'\n",
    "test_set = ImageNetDataTest(path, \"test.txt\", nclass)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test: 100/10000\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "preds = []\n",
    "preds_val = {}\n",
    "for i, data in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        name = data[0]\n",
    "        inputs = data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds_val[name] = outputs.cpu().numpy()\n",
    "        pred = outputs.argmax(axis=1).squeeze()\n",
    "        names.extend(name)\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"Test: {}/{}\".format(i + 1, len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([names,preds]).transpose()\n",
    "df.columns = ['Id', 'Category']\n",
    "df.loc[:, 'Id']=df['Id'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission_wideresnet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"prediction_wideresnet.pkl\", \"wb\")\n",
    "pickle.dump(preds_val, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(loader, fname, verbose, total):\n",
    "    preds_val = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader):\n",
    "            inputs = data[0].to(device)\n",
    "            targets = data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds_val.append((targets.cpu().numpy(), outputs.cpu().numpy()))\n",
    "            if (i + 1) % verbose == 0:\n",
    "                print(\"Predict: {}/{}\".format(i + 1, total))\n",
    "            torch.cuda.empty_cache()\n",
    "    f = open(fname, \"wb\")\n",
    "    pickle.dump(preds_val, f)\n",
    "    f.close()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predict: 100/1250\n",
      "Predict: 200/1250\n",
      "Predict: 300/1250\n",
      "Predict: 400/1250\n",
      "Predict: 500/1250\n",
      "Predict: 600/1250\n",
      "Predict: 700/1250\n",
      "Predict: 800/1250\n",
      "Predict: 900/1250\n",
      "Predict: 1000/1250\n",
      "Predict: 1100/1250\n",
      "Predict: 1200/1250\n",
      "Predict: 1300/1250\n",
      "Predict: 1400/1250\n",
      "Predict: 1500/1250\n"
     ]
    }
   ],
   "source": [
    "getPrediction(train_loader, \"prediction_wideresnet_train.pkl\", 100, int(len(train_set)/64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predict: 100/125\n"
     ]
    }
   ],
   "source": [
    "getPrediction(val_loader, \"prediction_wideresnet_val.pkl\", 100, int(len(val_set)/64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}